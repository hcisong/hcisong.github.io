<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Song HCI</title>
  <meta name="description" content="A researcher x developer × designer in XR, Web, HCI">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
  <link href="https://fonts.googleapis.com/css?family=Ubuntu:300,400&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Baloo+2&display=swap" rel="stylesheet">
  <!-- <link href="https://fonts.googleapis.com/css?family=Cormorant+Garamond&display=swap" rel="stylesheet"> -->
  <link href="https://fonts.googleapis.com/css?family=Julius+Sans+One&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="/projects.html">
  <link rel="alternate" type="application/rss+xml" title="Song HCI" href="/feed.xml">
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>


  
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper">


    <a class="site-title" href="/">
        <!-- <div class="logo"> -->
        <img class="logo-img" src="/assets/img/song-logo.svg">
        <span class="logo-text">SONG</span>
      <!-- </div> -->
    </a>

    <nav class="site-nav">
      <span class="menu-icon">
        <svg viewBox="0 0 18 15" width="18px" height="15px">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </span>

      <div class="trigger">

        <!-- 
          
        
          
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
          
        
          
         -->

        <a class="nav-page nav-projects" href="/projects.html">Projects</a>
        <a class="nav-page nav-about" href="/about.html">About me</a>
      </div>
    </nav>

  </div>

</header>
<main class="container" aria-label="Content">
      <!-- <div class="wrapper"> -->
        <div class="home" id="projects">
  <div class="projectsCollection">
    <div class="row">
      <div class="projects-title col-12 offset-lg-1 col-lg-10">
        <h1>My Projects</h1>
      </div>
    </div>




    <div class="row ">
      
      <div class="col-12 offset-lg-1 col-lg-10">
        <div class="row">
          <div class="col-12">
            <p class="projects-intro">This page lists my projects, including publications, prototypes, and other
          implementations. For more details, please refer to the publication links or <a href="mailto:hcisong@gmail.com">contact me</a>.</p>
          </div>
        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title">HapticWings: Enhancing the Experience of Extra Wing Motions in Virtual Reality
              through Dynamic 2D Weight Shifting</h3>
            <p class="project-subtitle">
              Yingjie Chang, Junyu Chen, Yilong Lin, Xuesong Zhang, Seungwoo Je
              <br>
              <i>In Proceedings of the 2025 ACM Designing Interactive Systems Conference (DIS 2025)<br>
                <a href="https://doi.org/10.1145/3715336.3735755">
                  https://doi.org/10.1145/3715336.3735755
                </a></i>
              <br>&#127942; <i class="note">BEST PAPER AWARD</i>

            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn " data-toggle="collapse" href="#dis25" role="button" aria-expanded="false"
              aria-controls="dis25">
              Abstract
            </a>
            <div class="collapse" id="dis25">
              <div class="card card-body">
                In virtual reality (VR), our virtual body can have different characteristics from our real body, such as
                appearance, size, and even extra body parts. Previous research shows that haptic feedback enhances the
                user-perceived embodiment of those dissimilar avatars. In particular, weight-shifting devices showed the
                potential to enhance arm deformation. However, there has been no exploration of using such techniques to
                enhance embodiment with extra body parts, like wings. We introduce HapticWings, a back-wearable 2D
                weight-shifting device that provides haptic feedback for wing motions, enhancing the user embodiment of
                avatars with extra wings. In three user studies, we explored (1) users’ abilities to recognize different
                weight-shifting motions provided by HapticWings, (2) users’ perceived embodiment of avatars with extra
                wings when providing haptic feedback for wing motions, and (3) four possible applications and used two
                of them to evaluate users’ sense of realism and enjoyment in VR.
              </div>
            </div>
          </div>
        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title">VRCaptions: Design Captions for DHH Users in Multiplayer Communication in VR</h3>
            <p class="project-subtitle">
              Tianze Xie*, Xuesong Zhang*, Feiyu Huang, Di Liu, Pengcheng An, Seungwoo Je (*Co-first)
              <br>
              <i>In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25)<br>
                <a href="https://doi.org/10.1145/3706598.3714186">
                  https://doi.org/10.1145/3706598.3714186
                </a>
                <!-- <br>
                <a href="https://doi.org/10.1145/3652988.3673918"
                  target="_blank">https://doi.org/10.1145/3652988.3673918</a> -->
              </i>
            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn " data-toggle="collapse" href="#chi25" role="button" aria-expanded="false"
              aria-controls="chi25">
              Abstract
            </a>
            <div class="collapse" id="chi25">
              <div class="card card-body">
                Accessing auditory information remains challenging for DHH individuals in real-world situations and
                multiplayer VR interactions. To improve this, we investigated caption designs that specialize in the
                needs of DHH users in multiplayer VR settings. First, we conducted three co-design workshops with DHH
                participants, social workers, and designers to gather insights into the specific needs of design
                directions for DHH users in the context of a room escape game in VR. We further refined our designs with
                13 DHH users to determine the most preferred features. Based on this, we developed VRCaptions, a caption
                prototype for DHH users to better experience multiplayer conversations in VR. We lastly invited two
                mixed-hearing groups to participate in the VR room escape game with our VRCaptions to validate. The
                results demonstrate that VRCaptions can enhance the ability of DHH participants to access information
                and reduce the barrier to communication in VR.
              </div>
            </div>
          </div>
        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title">Construction of SVS: Scale of Virtual Twin's Similarity to Physical Counterpart in
              Simple Environments
            </h3>
            <p class="project-subtitle">
              Xuesong Zhang, Adalberto L. Simeone
              <br>
              <i>In Proceedings of the 2024 ACM Symposium on Spatial User Interaction (SUI 2024). Association for
                Computing Machinery, Article 3, 9 pages.
                <br><a href="https://doi.org/10.1145/3677386.3682100"
                  target="_blank">https://doi.org/10.1145/3677386.3682100</a></i>
            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn " data-toggle="collapse" href="#sui24" role="button" aria-expanded="false"
              aria-controls="sui24">
              Abstract
            </a>
            <div class="collapse" id="sui24">
              <div class="card card-body">
                Due to the lack of a universally accepted definition for the term “virtual twin”, there are varying
                degrees of similarity between physical prototypes and their virtual counterparts across different
                research papers. This variability complicates the comparison of results from these papers. To bridge
                this gap, we introduce the Scale of Virtual Twin’s Similarity (SVS), a questionnaire intended to
                quantify the similarity between a virtual twin and its physical counterpart in simple environments in
                terms of visual fidelity, physical fidelity, environmental fidelity, and functional fidelity. This paper
                describes the development process of the SVS questionnaire items and provides an initial evaluation
                through two between-subjects user studies to validate the items under the categories of visual and
                functional fidelity. Additionally, we discuss the way to apply it in research and development settings.
              </div>
            </div>
          </div>
        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title">Focus Agent: LLM-Powered Virtual Focus Group
            </h3>
            <p class="project-subtitle">
              Taiyu Zhang, Xuesong Zhang, Robbe Cools, and Adalberto L. Simeone
              <br>
              <i>In ACM International Conference on Intelligent Virtual Agents (IVA 2024). ACM, 1-10.
                <br><a href="https://doi.org/10.1145/3652988.3673918"
                  target="_blank">https://doi.org/10.1145/3652988.3673918</a></i>
            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn " data-toggle="collapse" href="#iva24" role="button" aria-expanded="false"
              aria-controls="iva24">
              Abstract
            </a>
            <div class="collapse" id="iva24">
              <div class="card card-body">
                In the domain of Human-Computer Interaction, focus groups represent a widely utilised yet
                resource-intensive methodology, often demanding the expertise of skilled moderators and meticulous
                preparatory efforts. This study introduces the “Focus Agent,” a Large Language Model (LLM) powered
                framework that simulates both the focus group (for data collection) and acts as a moderator in a focus
                group setting with human participants. To assess the data quality derived from the Focus Agent, we ran
                five focus group sessions with a total of 23 human participants as well as deploying the Focus Agent to
                simulate these discussions with AI participants. Quantitative analysis indicates that Focus Agent can
                generate opinions similar to those of human participants. Furthermore, the research exposes some
                improvements associated with LLMs acting as moderators in focus group discussions that include human
                participants.
              </div>
            </div>
          </div>
        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title">ARcoustic: A Mobile Augmented Reality System for Seeing Out-of-View Traffic
            </h3>
            <p class="project-subtitle">
              Xuesong Zhang, Xian Wu, Robbe Cools, Adalberto L. Simeone, Uwe Gruenefeld
              <br>
              <i>In Proceedings of the 15th International Conference on Automotive User Interfaces and Interactive
                Vehicular Applications (AutomotiveUI 2023). 
                <br><a href="https://doi.org/10.1145/3580585.3606461"
                  target="_blank">https://doi.org/10.1145/3580585.3606461</a></i>
            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn " data-toggle="collapse" href="#autoui23" role="button" aria-expanded="false"
              aria-controls="autoui23">
              Abstract
            </a>
            <div class="collapse" id="autoui23">
              <div class="card card-body">
                Locating out-of-view vehicles can help pedestrians to avoid critical traffic encounters. Some previous
                approaches focused solely on visualising out-of-view objects, neglecting their localisation and
                limitations. Other methods rely on continuous camera-based localisation, raising privacy concerns.
                Hence, we propose the ARcoustic system, which utilises a microphone array for nearby moving vehicle
                localisation and visualises nearby out-of-view vehicles to support pedestrians. First, we present the
                implementation of our sonic-based localisation and discuss the current technical limitations. Next, we
                present a user study (n = 18) in which we compared two state-of-the-art visualisation techniques
                (Radar3D, CompassbAR) to a baseline without any visualisation. Results show that both techniques present
                too much information, resulting in below-average user experience and longer response times. Therefore,
                we introduce a novel visualisation technique that aligns with the technical localisation limitations and
                meets pedestrians’ preferences for effective visualisation, as demonstrated in the second user study (n
                = 16). Lastly, we conduct a small field study (n = 8) testing our ARcoustic system under realistic
                conditions. Our work shows that out-of-view object visualisations must align with the underlying
                localisation technology and fit the concrete application scenario.
              </div>
            </div>
          </div>
        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title">CReST: Design and Evaluation of the Cross-Reality Study Tool</h3>
            <p class="project-subtitle">
              Robbe Cools, Xuesong Zhang, Adalberto L. Simeone
              <br>
              <i>In Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia (MUM 2023).
                <br><a href="https://doi.org/10.1145/3626705.3627803"
                  target="_blank">https://doi.org/10.1145/3626705.3627803</a></i>
            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn " data-toggle="collapse" href="#mum23" role="button" aria-expanded="false"
              aria-controls="mum23">
              Abstract
            </a>
            <div class="collapse" id="mum23">
              <div class="card card-body">
                In this work, we describe our experience developing and evaluating the Cross-Reality Study Tool (CReST),
                which allows researchers to conduct and observe Virtual Reality (VR) user studies from an Augmented
                Reality perspective. So far, most research on conducting VR user studies has centred around tools for
                asynchronous setup, data collection and analysis, or (immersive) replays. Conversely, CReST is centred
                around supporting the researcher synchronously. We replicated three VR studies as example cases, applied
                CReST to them, and conducted an interview with one author of each case. We then performed a case study,
                and recruited 17 participants to take part in a user study where the researchers used CReST to observe
                participant interaction with virtual drawer and closet artefacts. We make CReST available for other
                researchers, as a tool to enable direct observation of participants in VR, and perform rapid,
                qualitative evaluations.
              </div>
            </div>
          </div>
        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title">Using the Think Aloud Protocol in an Immersive Virtual Reality Evaluation of a
              Virtual Twin
            </h3>
            <p class="project-subtitle">
              Xuesong Zhang, Adalberto L. Simeone
              <br>
              <i>In Proceedings of the 2022 ACM Symposium on Spatial User Interaction (SUI 2022). 
                <br><a href="https://doi.org/10.1145/3565970.3567706"
                  target="_blank">https://doi.org/10.1145/3565970.3567706</a></i>
            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn " data-toggle="collapse" href="#sui22" role="button" aria-expanded="false"
              aria-controls="sui22">
              Abstract
            </a>
            <a class="btn " data-toggle="collapse" href="#sui22-video" role="button" aria-expanded="false"
              aria-controls="sui22-video">
              Video
            </a>
            <div class="collapse" id="sui22">
              <div class="card card-body">
                Employing virtual prototypes and immersive Virtual Reality (VR) in usability evaluation can save time
                and speed up the iteration process during the design process. However, it is still unclear whether we
                can use conventional usability evaluation methods in VR and obtain results comparable to performing the
                evaluation on a physical prototype. Hence, we conducted a user study with 24 participants, where we
                compared the results obtained by using the Think Aloud Protocol to inspect an everyday product and its
                virtual twin. Results show that more than 60% of the reported usability problems were shared by both the
                physical and virtual prototype, and the in-depth qualitative analysis further highlights the potential
                of immersive VR evaluations. We report on the lessons we learned for designing and implementing virtual
                prototypes in immersive VR evaluations.
              </div>
            </div>
          </div>
          <div class="col-12 collapse" id="sui22-video">
            <div class="video-container">
              <iframe class="video" src="https://www.youtube.com/embed/ulbXAk0LsfY" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            </div>
          </div>

        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title">Using Heuristic Evaluation in Immersive Virtual Reality Evaluation
            </h3>
            <p class="project-subtitle">
              Xuesong Zhang, Adalberto L. Simeone
              <br>
              <i>MUM '21: Proceedings of the 20th International Conference on Mobile and Ubiquitous Multimedia
                <br><a href="https://doi.org/10.1145/3490632.3497863"
                  target="_blank">https://doi.org/10.1145/3490632.3497863
                </a></i>
            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn btn-primary" data-toggle="collapse" href="#mum21" role="button" aria-expanded="false"
              aria-controls="mum21">
              Abstract
            </a>
            <a class="btn btn-primary" data-toggle="collapse" href="#mum21-pdf" role="button" aria-expanded="false"
              aria-controls="mum21-pdf">
              Poster
            </a>
            <div class="collapse" id="mum21">
              <div class="card card-body">
                Previous works show that virtual reality itself can be used as a medium in which to stage an
                experimental evaluation. However, it is still unclear whether conventional usability evaluation methods
                can directly be applied to virtual reality evaluations and whether they will lead to similar insights
                when compared to equivalent real-world lab studies. Therefore, we conducted a user study with nine
                participants, comparing Heuristic Evaluation (HE) for the evaluations of a novel smart artefact. We
                asked participants to evaluate the physical prototype and their virtual counterparts in the real-world
                and the virtual environment, respectively. Results show the HE have similar performance when evaluating
                artefacts usability in VR and real-world in terms of identified usability problems. The VR
                implementation has an impact on the immersive VR evaluation result.
              </div>
            </div>
          </div>
          <div class="col-12 collapse" id="mum21-pdf">
            <div class="pdf-container">
              <embed src="assets/file/MUM-poster.pdf" width="500" height="760" type="application/pdf">
            </div>
          </div>

        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title">Shunfeng’er: A portable solution with Huawei Eyewear to enhance your hearing
              capability</h3>
            <p class="project-subtitle">
              Xuesong Zhang, Xian Wu, Fei Qu, Adalberto L. Simeone
              <br>
              <i>MobileHCI '22: 24th International Conference on Human-Computer Interaction with Mobile Devices and
                Services </i>
              <br>&#127942; <i class="note">Most Creative Award in Student Design Competition</i>
              </i>
            </p>
          </div>
          <div class="project-desc col-12">

            <a class="btn" data-toggle="collapse" href="#MobileHCI22-video" role="button" aria-expanded="false"
              aria-controls="MobileHCI22-video">
              Video
            </a>
          </div>
          <div class="col-12 collapse" id="MobileHCI22-video">
            <div class="video-container">
              <iframe class="video" src="https://www.youtube.com/embed/E2lJgYcYeIU" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            </div>
          </div>

        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title ">Assessing Social Text Placement in Mixed Reality TV</h3>
            <p class="project-subtitle">
              Florian Mathis, Xuesong Zhang, Mark McGill, Adalberto L. Simeone, Mohamed Khamis
              <br>
              <i>IMX '20: ACM International Conference on Interactive Media Experiences
                <br><a href="https://doi.org/10.1145/3391614.3399402"
                  target="_blank">https://doi.org/10.1145/3391614.3399402</a></i>
              <br>&#127942; <i class="note">BEST WORK-IN-PROGRESS AWARD</i>
              </i>
            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn btn-primary" data-toggle="collapse" href="#imx20" role="button" aria-expanded="false"
              aria-controls="imx20">
              Abstract
            </a>
            <a class="btn btn-primary" data-toggle="collapse" href="#imx20-video" role="button" aria-expanded="false"
              aria-controls="imx20-video">
              Video
            </a>
            <div class="collapse" id="imx20">
              <div class="card card-body">
                TV experiences are often social, be it at-a-distance (through text) or in-person (through speech). Mixed
                Reality (MR) headsets offer new opportunities to enhance social communication during TV viewing by
                placing social artifacts (e.g. text) anywhere the viewer wishes, rather than being constrained to a
                smartphone or TV display. In this paper, we use VR as a test-bed to evaluate different text locations
                for MR TV specifically. We introduce the concepts of wall messages, below-screen messages, and
                egocentric messages in addition to state-of-the-art on-screen messages (i.e., subtitles) and controller
                messages (i.e., reading text messages on the mobile device) to convey messages to users during TV
                viewing experiences. Our results suggest that a) future MR systems that aim to improve viewers’
                experience need to consider the integration of a communication channel that does not interfere with
                viewers’ primary task, that is watching TV, and b) independent of the location of text messages, users
                prefer to be in full control of them, especially when reading and responding to them. Our findings pave
                the way for further investigations towards social at-a-distance communication in Mixed Reality.
              </div>
            </div>
          </div>
          <div class="col-12 collapse" id="imx20-video">
            <div class="video-container">
              <iframe class="video" src="https://www.youtube.com/embed/aNCXm7MYLVk" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            </div>
          </div>

        </div>


        <div class="row project">
          <div class="col-12">
            <h3 class="project-title ">Outline Pursuits: Gaze-assisted Selection of Occluded Objects in virtual Reality
            </h3>
            <p class="project-subtitle">
              Ludwig Sidenmark, Christopher Clarke, Xuesong Zhang, Jenny Phu, Hans Gellersen
              <br>
              <i>CHI '20: CHI Conference on Human Factors in Computing Systems
                <br><a href="https://doi.org/10.1145/3313831.3376438"
                  target="_blank">https://doi.org/10.1145/3313831.3376438</a>
              </i>
            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn btn-primary" data-toggle="collapse" href="#chi20" role="button" aria-expanded="false"
              aria-controls="chi20">
              Abstract
            </a>
            <a class="btn btn-primary" data-toggle="collapse" href="#chi20-video" role="button" aria-expanded="false"
              aria-controls="chi20-video">
              Video
            </a>
            <div class="collapse" id="chi20">
              <div class="card card-body">
                In 3D environments, objects can be difficult to select when they overlap, as this affects available
                target area and increases selection ambiguity. We introduce Outline Pursuits which extends a primary
                pointing modality for gaze-assisted selection of occluded objects. Candidate targets within a pointing
                cone are presented with an outline that is traversed by a moving stimulus. This affords completion of
                the selection by gaze attention to the intended target's outline motion, detected by matching the user's
                smooth pursuit eye movement. We demonstrate two techniques implemented based on the concept, one with a
                controller as the primary pointer, and one in which Outline Pursuits are combined with head pointing for
                hands-free selection. Compared with conventional raycasting, the techniques require less movement for
                selection as users do not need to reposition themselves for a better line of sight, and selection time
                and accuracy are less affected when targets become highly occluded.
              </div>
            </div>
          </div>
          <div class="col-12 collapse" id="chi20-video">
            <div class="video-container">
              <iframe class="video" src="https://www.youtube.com/embed/vKxNc4PXD14" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            </div>
          </div>

        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title ">Cloud Columba CC2</h3>
            <p class="project-subtitle"><i>Designing and implementing the <a href="https://cc2.cloud-columba.org/"
                  target="_blank">Cloud Columba CC2</a>
                09.2019</i></p>
          </div>
          <div class="project-desc col-12">
            <a class="btn btn-primary" data-toggle="collapse" href="#cc19" role="button" aria-expanded="false"
              aria-controls="cc19">
              Description
            </a>
            <a class="btn btn-primary" data-toggle="collapse" href="#cc19-video" role="button" aria-expanded="false"
              aria-controls="cc19-video">
              Video
            </a>
            <div class="collapse" id="cc19">
              <div class="card card-body">
                The Cloud Columba CC2 is a web application for the design and simulation of microfluidic systems. I was
                a member of Columba Team, designed and implemented the Cloud Columba CC2.

              </div>
            </div>
          </div>
          <div class="col-12 collapse" id="cc19-video">
            <div class="video-container">
              <iframe class="video" src="https://www.youtube.com/embed/i4KMXrke_oE" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>

        <div class="row project">
          <div class=" col-12">
            <h3 class="project-title">Intelligent Information Lighting at Subway Stations</h3>
            <p class="project-subtitle"><i>Project of Designworkshop II SS18: Escaping Flatland 07.2018</i></p>
          </div>
          <div class="project-desc col-12">
            <a class="btn btn-primary" data-toggle="collapse" href="#LightStation" role="button" aria-expanded="false"
              aria-controls="LightStation">
              Description
            </a>
            <a class="btn btn-primary" data-toggle="collapse" href="#LightStation-video" role="button"
              aria-expanded="false" aria-controls="LightStation-video">
              Video
            </a>
            <div class="collapse" id="LightStation">
              <div class="card card-body">
                The subway is a crucial mode of transportation connecting different areas of a city, but aging
                infrastructure has resulted in uneven distribution of passengers in different sections of incoming
                trains. To address this issue, an Intelligent Information Lighting System has been implemented at subway
                stations to help passengers predict train load levels and improve their experience. This project,
                carried out by an interdisciplinary team including students from Human-Computer Interaction, Landscape
                Architecture and Media Informatics, utilized an iterative design process to tackle a real-world urban
                challenge.
              </div>
            </div>

          </div>
          <div class="col-12 collapse" id="LightStation-video">
            <div class="video-container">
              <iframe
                src="https://kuleuven-my.sharepoint.com/personal/xuesong_zhang_kuleuven_be/_layouts/15/embed.aspx?UniqueId=4f3844c1-2d55-4825-9ce1-43d7a5c3a033&embed=%7B%22ust%22%3Atrue%7D&referrer=OneUpFileViewer&referrerScenario=EmbedDialog.Create"
                width="640" height="360" frameborder="0" scrolling="no" allowfullscreen title="Escaping Flatlands.mp4"
                style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;"></iframe>
            </div>
          </div>
        </div>
        <div class="row project">
          <div class=" col-12">
            <h3 class="project-title">Finde heraus, ob du das Zeug für ein Onlinemedien-Studium hast!</h3>
            <p class="project-subtitle"><i>Project of Media production. 07.2017</i></p>
          </div>
          <div class="project-desc col-12">
            <a class="btn btn-primary" data-toggle="collapse" href="#dhbw" role="button" aria-expanded="false"
              aria-controls="dhbw">
              Description
            </a>
            <a class="btn btn-primary" data-toggle="collapse" href="#dhbw-video" role="button" aria-expanded="false"
              aria-controls="dhbw-video">
              Video
            </a>
            <div class="collapse" id="dhbw">
              <div class="card card-body">
                This short film was created as part of a media production course.
              </div>
            </div>

          </div>
          <div class="col-12 collapse" id="dhbw-video">
            <div class="video-container">
              <iframe class="video" src="https://www.youtube.com/embed/UAkX6YK94wU" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>

            </div>
          </div>
        </div>
      </div>


    </div>




  </div>



</div>


</div>
      <!-- </div> -->
    </main><footer class="site-footer">

  <div class="container">

    <!-- <h2 class="footer-heading">Song HCI</h2> -->

    <div class="row">
      <div class="col-12 offset-lg-6 col-lg-5">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/song-what" target="_blank">
  <span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span>
  <!-- <span class="username">song-what</span> -->
</a>

          </li>
          

          

          <li>
            <a href="https://www.linkedin.com/in/xuesongzhang/" target="_blank">
  <span class="icon icon--linkedin"><svg id="Bold" enable-background="new 0 0 24 24" height="512" viewBox="0 0 24 24" width="512" xmlns="http://www.w3.org/2000/svg"><path d="m23.994 24v-.001h.006v-8.802c0-4.306-.927-7.623-5.961-7.623-2.42 0-4.044 1.328-4.707 2.587h-.07v-2.185h-4.773v16.023h4.97v-7.934c0-2.089.396-4.109 2.983-4.109 2.549 0 2.587 2.384 2.587 4.243v7.801z"/><path d="m.396 7.977h4.976v16.023h-4.976z"/><path d="m2.882 0c-1.591 0-2.882 1.291-2.882 2.882s1.291 2.909 2.882 2.909 2.882-1.318 2.882-2.909c-.001-1.591-1.292-2.882-2.882-2.882z"/></svg></span>
  <!-- <span class="username"></span> -->
</a>

          </li>
        </ul>
        <!-- <p>Social Media</p> -->
        <ul class="contact-list">

          
          <li><a href="mailto:hcisong@gmail.com">hcisong@gmail.com</a></li>
          
          <!-- <div class="footer-col footer-col-3"> -->
            <li>
              A researcher x developer × designer in XR, Web, HCI
            </li>
          <!-- </div> -->
      </ul>
        
      </div>


    </div>

  </div>

</footer>
</body>

</html>
