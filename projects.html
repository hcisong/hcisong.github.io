---
layout: default
---
<!-- <div class="row">
  <div class="logo col-6">

    <img class="mylogo" src="assets/img/song-logo.svg">
    <span>Xuesong Zhang</span>
  </div>
</div> -->
<div class="home" id="projects">
  <div class="projectsCollection">
    <div class="row">
      <div class="projects-title offset-2 col-8">
        <h1>My Projects</h1>
      </div>
    </div>


   



    <div class="row ">
      <div class="offset-2 col-8">
        <div class="row project">
          <div class="col-12">
            <h3 class="project-title">Using the Think Aloud Protocol in an Immersive Virtual Reality Evaluation of a Virtual Twin
            </h3>
            <p class="project-subtitle">
              Xuesong Zhang, Adalberto L. Simeone           
              <br>
              <i>SUI '22: Proceedings of the 2022 ACM Symposium on Spatial User Interaction
              <br><a href="https://doi.org/10.1145/3565970.3567706"  target="_blank">https://doi.org/10.1145/3565970.3567706</a></i>
            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn " data-toggle="collapse" href="#sui22" role="button" aria-expanded="false" aria-controls="sui22">
              Abstract
            </a>
            <a class="btn " data-toggle="collapse" href="#sui22-video" role="button" aria-expanded="false" aria-controls="sui22-video">
              Video
            </a>
            <div class="collapse" id="sui22">
              <div class="card card-body">
                Employing virtual prototypes and immersive Virtual Reality (VR) in usability evaluation can save time and speed up the iteration process during the design process. However, it is still unclear whether we can use conventional usability evaluation methods in VR and obtain results comparable to performing the evaluation on a physical prototype. Hence, we conducted a user study with 24 participants, where we compared the results obtained by using the Think Aloud Protocol to inspect an everyday product and its virtual twin. Results show that more than 60% of the reported usability problems were shared by both the physical and virtual prototype, and the in-depth qualitative analysis further highlights the potential of immersive VR evaluations. We report on the lessons we learned for designing and implementing virtual prototypes in immersive VR evaluations.
              </div>
            </div>
          </div>
          <div class="col-12 collapse" id="sui22-video">
            <div class="video-container">
              <iframe class="video" src="https://www.youtube.com/embed/ulbXAk0LsfY" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            </div>
          </div>

        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title">Using Heuristic Evaluation in Immersive Virtual Reality Evaluation
            </h3>
            <p class="project-subtitle">
              Xuesong Zhang, Adalberto L. Simeone           
              <br>
              <i>MUM '21: Proceedings of the 20th International Conference on Mobile and Ubiquitous Multimedia 
              <br><a href="https://doi.org/10.1145/3490632.3497863"  target="_blank">https://doi.org/10.1145/3490632.3497863
              </a></i>
            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn btn-primary" data-toggle="collapse" href="#mum21" role="button" aria-expanded="false" aria-controls="mum21">
              Abstract
            </a>
            <a class="btn btn-primary" data-toggle="collapse" href="#mum21-pdf" role="button" aria-expanded="false" aria-controls="mum21-pdf">
              Poster
            </a>
            <div class="collapse" id="mum21">
              <div class="card card-body">
                Previous works show that virtual reality itself can be used as a medium in which to stage an experimental evaluation. However, it is still unclear whether conventional usability evaluation methods can directly be applied to virtual reality evaluations and whether they will lead to similar insights when compared to equivalent real-world lab studies. Therefore, we conducted a user study with nine participants, comparing Heuristic Evaluation (HE) for the evaluations of a novel smart artefact. We asked participants to evaluate the physical prototype and their virtual counterparts in the real-world and the virtual environment, respectively. Results show the HE have similar performance when evaluating artefacts usability in VR and real-world in terms of identified usability problems. The VR implementation has an impact on the immersive VR evaluation result.
              </div>
            </div>
          </div>
          <div class="col-12 collapse" id="mum21-pdf">
            <div class="pdf-container">
              <embed src="assets/file/MUM-poster.pdf" width="500" height="760" type="application/pdf">
            </div>
          </div>

        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title">Shunfeng’er:  A portable solution with Huawei Eyewear to enhance your hearing capability</h3>
            <p class="project-subtitle">
              Xuesong Zhang, Xian Wu, Fei Qu, Adalberto L. Simeone        
              <br>
              <i>MobileHCI '22: 24th International Conference on Human-Computer Interaction with Mobile Devices and Services
                <br>Most Creative Award in Student Design Competition
                <br>
              </i>
            </p>
          </div>
          <div class="project-desc col-12">
            <!-- <a class="btn" data-toggle="collapse" href="#MobileHCI22" role="button" aria-expanded="false" aria-controls="MobileHCI22">
              Description
            </a> -->
            <a class="btn" data-toggle="collapse" href="#MobileHCI22-video" role="button" aria-expanded="false" aria-controls="MobileHCI22-video">
              Video
            </a>
            <!-- <div class="collapse" id="MobileHCI22">
              <div class="card card-body">
                todo
              </div>
            </div> -->
          </div>
          <div class="col-12 collapse" id="MobileHCI22-video">
            <div class="video-container">
              <iframe src="https://kuleuven-my.sharepoint.com/personal/xuesong_zhang_kuleuven_be/_layouts/15/embed.aspx?UniqueId=11f6e2ed-d525-4a25-a5f8-fb9a6578d22a&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=OneUpFileViewer&referrerScenario=EmbedDialog.Create" width="640" height="360" frameborder="0" scrolling="no" allowfullscreen title="Shunfenger_video.mp4" style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;">
              </iframe>
            </div>
          </div>

        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title ">Assessing Social Text Placement in Mixed Reality TV</h3>
            <p class="project-subtitle">
              Florian Mathis, Xuesong Zhang, Mark McGill, Adalberto L. Simeone, Mohamed Khamis
              <br>
              <i>IMX '20: ACM International Conference on Interactive Media Experiences
                <br><a href="https://doi.org/10.1145/3391614.3399402" target="_blank">https://doi.org/10.1145/3391614.3399402</a>
                <br>BEST WORK-IN-PROGRESS AWARD
              </i>
            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn btn-primary" data-toggle="collapse" href="#imx20" role="button" aria-expanded="false" aria-controls="imx20">
              Abstract
            </a>
            <a class="btn btn-primary" data-toggle="collapse" href="#imx20-video" role="button" aria-expanded="false" aria-controls="imx20-video">
              Video
            </a>
            <div class="collapse" id="imx20">
              <div class="card card-body">
                TV experiences are often social, be it at-a-distance (through text) or in-person (through speech). Mixed Reality (MR) headsets offer new opportunities to enhance social communication during TV viewing by placing social artifacts (e.g. text) anywhere the viewer wishes, rather than being constrained to a smartphone or TV display. In this paper, we use VR as a test-bed to evaluate different text locations for MR TV specifically. We introduce the concepts of wall messages, below-screen messages, and egocentric messages in addition to state-of-the-art on-screen messages (i.e., subtitles) and controller messages (i.e., reading text messages on the mobile device) to convey messages to users during TV viewing experiences. Our results suggest that a) future MR systems that aim to improve viewers’ experience need to consider the integration of a communication channel that does not interfere with viewers’ primary task, that is watching TV, and b) independent of the location of text messages, users prefer to be in full control of them, especially when reading and responding to them. Our findings pave the way for further investigations towards social at-a-distance communication in Mixed Reality.
              </div>
            </div>
          </div>
          <div class="col-12 collapse" id="imx20-video">
            <div class="video-container">
              <iframe class="video" src="https://www.youtube.com/embed/aNCXm7MYLVk" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            </div>
          </div>

        </div>


        <div class="row project">
          <div class="col-12">
            <h3 class="project-title ">Outline Pursuits: Gaze-assisted Selection of Occluded Objects in virtual Reality</h3>
            <p class="project-subtitle">
              Ludwig Sidenmark, Christopher Clarke, Xuesong Zhang, Jenny Phu, Hans Gellersen
              <br>
              <i>CHI '20: CHI Conference on Human Factors in Computing Systems
                <br><a href="https://doi.org/10.1145/3313831.3376438" target="_blank">https://doi.org/10.1145/3313831.3376438</a>
              </i>
            </p>
          </div>
          <div class="project-desc col-12">
            <a class="btn btn-primary" data-toggle="collapse" href="#chi20" role="button" aria-expanded="false" aria-controls="chi20">
              Abstract
            </a>
            <a class="btn btn-primary" data-toggle="collapse" href="#chi20-video" role="button" aria-expanded="false" aria-controls="chi20-video">
              Video
            </a>
            <div class="collapse" id="chi20">
              <div class="card card-body">
                In 3D environments, objects can be difficult to select when they overlap, as this affects available target area and increases selection ambiguity. We introduce Outline Pursuits which extends a primary pointing modality for gaze-assisted selection of occluded objects. Candidate targets within a pointing cone are presented with an outline that is traversed by a moving stimulus. This affords completion of the selection by gaze attention to the intended target's outline motion, detected by matching the user's smooth pursuit eye movement. We demonstrate two techniques implemented based on the concept, one with a controller as the primary pointer, and one in which Outline Pursuits are combined with head pointing for hands-free selection. Compared with conventional raycasting, the techniques require less movement for selection as users do not need to reposition themselves for a better line of sight, and selection time and accuracy are less affected when targets become highly occluded.
              </div>
            </div>
          </div>
          <div class="col-12 collapse" id="chi20-video">
            <div class="video-container">
              <iframe class="video" src="https://www.youtube.com/embed/vKxNc4PXD14" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            </div>
          </div>

        </div>

        <div class="row project">
          <div class="col-12">
            <h3 class="project-title ">Cloud Columba CC2</h3>
            <p class="project-subtitle"><i>Designing and implementing the <a href="https://cc2.cloud-columba.org/" target="_blank">Cloud Columba CC2</a>
                09.2019</i></p>
          </div>
          <div class="project-desc col-12">
            <a class="btn btn-primary" data-toggle="collapse" href="#cc19" role="button" aria-expanded="false" aria-controls="cc19">
              Description
            </a>
            <a class="btn btn-primary" data-toggle="collapse" href="#cc19-video" role="button" aria-expanded="false" aria-controls="cc19-video">
              Video
            </a>
            <div class="collapse" id="cc19">
              <div class="card card-body">
              The Cloud Columba CC2 is a web application for the design and simulation of microfluidic systems. I was a member of Columba Team, designed and implemented the Cloud Columba CC2.

              </div>
            </div>
          </div>
          <div class="col-12 collapse" id="cc19-video">
            <div class="video-container">
              <iframe class="video" src="https://www.youtube.com/embed/i4KMXrke_oE" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>

        <div class="row project">
          <div class=" col-12">
            <h3 class="project-title">Intelligent Information Lighting at Subway Stations</h3>
            <p class="project-subtitle"><i>Project of Designworkshop II SS18: Escaping Flatland 07.2018</i></p>
          </div>
          <div class="project-desc col-12">
              <a class="btn btn-primary" data-toggle="collapse" href="#LightStation" role="button" aria-expanded="false" aria-controls="LightStation">
                Description
              </a>
              <a class="btn btn-primary" data-toggle="collapse" href="#LightStation-video" role="button" aria-expanded="false" aria-controls="LightStation-video">
                Video
              </a>
              <div class="collapse" id="LightStation">
                <div class="card card-body">
              The subway is a crucial mode of transportation connecting different areas of a city, but aging infrastructure has resulted in uneven distribution of passengers in different sections of incoming trains. To address this issue, an Intelligent Information Lighting System has been implemented at subway stations to help passengers predict train load levels and improve their experience. This project, carried out by an interdisciplinary team including students from Human-Computer Interaction, Landscape Architecture and Media Informatics, utilized an iterative design process to tackle a real-world urban challenge.
              </div>
              </div>
            
          </div>
          <div class="col-12 collapse" id="LightStation-video">
            <div class="video-container">
              <iframe src="https://kuleuven-my.sharepoint.com/personal/xuesong_zhang_kuleuven_be/_layouts/15/embed.aspx?UniqueId=4f3844c1-2d55-4825-9ce1-43d7a5c3a033&embed=%7B%22ust%22%3Atrue%7D&referrer=OneUpFileViewer&referrerScenario=EmbedDialog.Create" width="640" height="360" frameborder="0" scrolling="no" allowfullscreen title="Escaping Flatlands.mp4" style="border:none; position: absolute; top: 0; left: 0; right: 0; bottom: 0; height: 100%; max-width: 100%;"></iframe> 
            </div>
          </div>
        </div>
      </div>
    </div>




  </div>



</div>






<!-- <p class="rss-subscribe">subscribe <a href="{{ "/feed.xml" | prepend: site.baseurl }}">via RSS</a></p> -->

</div>